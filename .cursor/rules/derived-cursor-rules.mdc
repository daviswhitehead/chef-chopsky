---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## HEADERS

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

*   **Environment Workflow and Testing Strategy:** Document the environment workflow, explaining the purpose of each environment (Development, Preview, Staging, and Production), the testing strategy for each, and the overall deployment workflow. Include environment-specific configurations and a cost-effective approach using free tiers.
*   **Staging Setup Guide:** Update the staging setup guide to reflect the separate Vercel project approach instead of using Vercel's built-in environment functionality. This includes architectural changes and step-by-step instructions for creating a separate staging project.
*   **Local Supabase via CLI:** Adopt Supabase CLI for local development database (Docker-based). Add `npm run supabase:start`/`supabase:stop` scripts and docs. Create `frontend/.env.local.example` pointing to local Supabase URL/anon key. Document migrations flow: `npx supabase db reset` for local; `db push` for staging/prod. Document environment mapping: Local (CLI) | Staging (hosted `chef-chopsky-staging`) | Production (hosted `chef-chopsky`). Note: the previous `chef-chopsky-local` hosted project has been repurposed as the staging project to stay within free tier limits. Document local Supabase setup and usage.
    *   **Environment Mapping:**
        *   **Local**: Supabase CLI (Docker) - `http://127.0.0.1:54321`
        *   **Staging**: Hosted Supabase project `chef-chopsky-staging`
        *   **Production**: Hosted Supabase project `chef-chopsky-production`

## TECH STACK

## CODING STANDARDS

## WORKFLOW & RELEASE RULES

*   **Vercel Environment Variables:** When setting environment variables in Vercel projects, the separate staging project approach uses the "production" environment type for the staging project. This is done to stay within free tier limits. This may impact the commands used to sync environment variables.
*   **Syncing Vercel Environment Variables:** The `sync-vercel-env.sh` script can be used to sync environment variables. The separate staging project uses the "production" environment type, so the command should be run with the `--environment production` flag.
    *   When syncing environment variables for the staging environment, first navigate to the frontend directory (`cd frontend`). Then, link the Vercel project (`vercel link --yes --project chef-chopsky-staging`). Finally, run the sync script from the parent directory (`cd ..; ./scripts/sync-vercel-env.sh --file frontend/.env.staging --environment production`).
*   **Feature Branch Staging Deployments**: The trigger for feature branch staging deployments uses the following logic:
    *   **Triggers**: The workflow is automatically triggered when code is pushed to any branch that starts with `feat/` or `feature/`.
    *   **URL Generation**: The deployment URL is created by taking the branch name (e.g., `feat/new-recipe-search`), replacing `/` with `-` (becomes `feat-new-recipe-search`), and creating the URL: `https://chef-chopsky-git-feat-new-recipe-search.vercel.app`.
*   **Pre-Commit Hooks Implementation:** This project uses [Husky](https://typicode.github.com/husky/) and [lint-staged](https://github.com/okonet/lint-staged) to automatically run linting checks before commits. The pre-commit hook configuration should include running `eslint --fix` first, followed by `eslint` to catch non-fixable errors, ensuring all ESLint errors are caught before committing. The pre-commit hook configuration should also include a TypeScript compilation check (`tsc --noEmit`) to catch type errors.
    *   To setup:
        ```bash
        npm install
        ```
    *   The lint-staged configuration is in `package.json`:
        ```json
        {
          "lint-staged": {
            "frontend/**/*.{js,jsx,ts,tsx}": [
              "cd frontend && npx eslint --fix",
              "cd frontend && npx eslint",
              "cd frontend && npx tsc --noEmit"
            ],
            "agent/src/**/*.{js,ts}": [
              "cd agent && npx eslint --fix",
              "cd agent && npx eslint",
              "cd agent && npx tsc --noEmit"
            ]
          }
        ```
*   **CI Performance Improvements:**
    *   **Caching:** Implement caching for npm dependencies, Playwright browsers, and Next.js build artifacts to reduce download and build times.
        *   Use `actions/setup-node` cache: `cache: 'npm'` for root, `frontend/`, and `agent/` installs.  Key the cache using `cache-dependency-path` to invalidate the cache when dependencies change.
        *   Cache Playwright browsers: `~/.cache/ms-playwright`. Key the cache using the Playwright version.
        *   Cache Next.js build output (`.next` or `.next-ci`) keyed on lockfile + `next.config.js` + source hash. Avoid caching failed build outputs.
    *   **Avoid Duplicate Builds**: Build once in the "Code Quality" job, then upload the build output. Download it in the "Integration & E2E" job instead of rebuilding.
    *   **Parallelization:** Split integration and E2E tests into parallel jobs to reduce the overall CI time. Create an E2E job to run E2E tests using a matrix strategy to shard tests across workers. Ensure each shard discovers the same test files from the same cwd/config to guarantee a consistent and non-overlapping test split. Use Playwright's `--shard n/m` option to split the test set deterministically across workers.
    *   **Test Scoping:** Implement test scoping to run only essential tests on pull requests and full tests on merges or scheduled builds.
*   **Test Execution and Orchestration**
    *   **Test Scoping**: Run unit and smoke integration tests on PRs; reserve full E2E tests for main/label.
    *   **E2E Test Marking**: Mark heavy/slow tests with `@slow` and exclude them from PRs.
    *   **Playwright WebServer**: Use Playwright's `webServer` to start and stop servers only for E2E tests, eliminating the need for separate scripts.
    *   **Test Splitting**: Shard Playwright tests across 2-3 workers or use `--project` splits to reduce test time.
*   **CI Time Optimization**
    *   **Avoid Duplicate Builds**: Build once in the "Code Quality" job, then upload the build output. Download it in the "Integration & E2E" job instead of rebuilding.
    *   **Supabase Startup**: Start Supabase once per job and tear down only at the job end. Avoid repeated `supabase status` calls; export the env once.
*   **Pre-commit Hooks**:
    *   Run `eslint --fix` first to auto-fix fixable issues.
    *   Then run `eslint` to catch non-fixable errors.
    *   Include a TypeScript compilation check (`tsc --noEmit`) to catch type errors.
*   **Next.js Build Process**:
    *   To address the Next.js build process and avoid the `Html should not be imported outside of pages/_document` error, the following configurations must be considered:
        *   **NODE_ENV Consistency**: Use `NODE_ENV=production` specifically for the build step.
        *   **Dynamic Rendering**: Apply `export const dynamic = 'force-dynamic'` to error pages to avoid static generation.
        *   **Separate Build Directory**: Use a dedicated build directory (e.g., `.next-ci`) for CI builds to prevent conflicts.
        * Custom Error Pages**: Removing custom error pages (`error.tsx` and `not-found.tsx`) may be necessary, allowing Next.js to handle them with its built-in functionality.
*   **Harden frontend chat API**: guard fetch, safe logging, single retry on transient errors
    *   When performing a fetch to the agent service, ensure the fetch is guarded with a try/catch, and the code performs safe logging.
    *   Implement a single retry on transient errors like AbortError or ECONNREFUSED.
        ```typescript
         // Perform the agent fetch with a single retry on transient errors
         async function fetchAgentOnce(): Promise<Response> {
           return await fetch(`${AGENT_SERVICE_URL}/chat`, {
             method: 'POST',
             headers: { 'Content-Type': 'application/json' },
             body: JSON.stringify({
               conversation_id: conversationId,
               messages: filtered,
               client_metadata: {
                 user_id: userId ?? null,
                 source: 'frontend',
                 user_agent: request.headers.get('user-agent') || 'unknown'
               }
             }),
             // Use centralized timeout configuration; do not assume it's always set
             signal: AbortSignal.timeout(getEnvironmentTimeouts().API_ROUTE || 15000)
           })
         }

         let agentResponse: Response | undefined
         try {
           agentResponse = await fetchAgentOnce()
         } catch (err: any) {
           // Retry once on common transient errors
           const transient = err?.name === 'AbortError' || err?.code === 'ECONNREFUSED' || err?.code === 'ENOTFOUND' || err?.message?.includes('fetch failed')
           if (transient) {
             // small backoff
             await new Promise(r => setTimeout(r, 200))
             try {
               agentResponse = await fetchAgentOnce()
             } catch (err2) {
               const agentDurationErr = Date.now() - agentStartTime;
               console.error(`[${requestId}] ðŸ’¥ Agent fetch failed after retry in ${agentDurationErr}ms:`, err2)
             }
           } else {
             const agentDurationErr = Date.now() - agentStartTime;
             console.error(`[${requestId}] ðŸ’¥ Agent fetch failed (non-transient) in ${agentDurationErr}ms:`, err)
           }
         }
         if (agentResponse) {
           console.log(`[${requestId}] â±ï¸ Agent service responded in ${agentResponse.status}`);
         } else {
           console.error(`[${requestId}] ðŸ’¥ Agent service did not return a response after ${agentDuration}ms`)
           return NextResponse.json(
             { error: 'Agent unavailable', message: 'Failed to reach agent service' },
             { status: 502 }
           )
         }

         if (!agentResponse.ok) {
           const err = await safeJson(agentResponse)
           console.log(`[${requestId}] âŒ Agent service error:`, err);

           // Persist error message to Supabase
           console.log(`[${requestId}] ðŸ’¾ Persisting error message to Supabase`);
           try {
             const errorPersistStart = Date.now();
             const persistResponse = await supabaseClient
               .from('conversation_messages')
               .insert([
                 {
                   conversation_id: conversationId,
                   role: 'system',
                   content: `Agent service error: ${err?.message || 'Unknown error'}`,
                   user_id: userId,
                   source: 'system',
                   created_at: new Date(),
                   message_id: `error-${messageId}` // Unique ID for the error message
                 }
               ]);
             if (persistResponse.error) {
               console.error(`[${requestId}] âŒ Supabase persist error:`, persistResponse.error);
             } else {
               const persistDuration = Date.now() - errorPersistStart;
               console.log(`[${requestId}] âœ… Error message persisted (duration: ${persistDuration}ms)`);
             }
           } catch (persistError) {
             console.error(`[${requestId}] âŒ Error persisting assistant message:`, persistError);
           }
           return NextResponse.json({
             error: 'Agent processing failed',
             message: err?.message || 'Something went wrong',
             timing_ms
           }, { status: 500 });
         }
         const data = await agentResponse.json();
         console.log(`[${requestId}] ðŸ¤– Agent service returned data:`, data);

         const assistantMessage = data.assistant_message;

         if (!assistantMessage) {
           console.error(`[${requestId}] âŒ Agent service returned invalid data:`, data);
           return res.status(500).json({
             error: 'Agent processing failed',
             message: 'Invalid agent response',
             timing_ms
           });
         }

         // Persist agent message to Supabase
         console.log(`[${requestId}] ðŸ’¾ Persisting assistant message to Supabase`);
         try {
           const persistStart = Date.now();
           const persistResponse = await supabaseClient
             .from('conversation_messages')
             .insert([
               {
                 conversation_id: ConversationId,
                 role: assistantMessage.role,
                 content: assistantMessage.content,
                 user_id: userId,
                 created_at: new Date(assistantMessage.created_at ?? Date.now()),
                 message_id: assistantMessage.id,
                 source: 'agent',
                 usage: assistantMessage.usage,
                 model: assistantMessage.model,
               }
             ]);

           if (persistResponse.error) {
             console.error(`[${requestId}] âŒ Supabase persist error:`, persistResponse.error);
           } else {
             const persistDuration = Date.now() - persistStart;
             console.log(`[${requestId}] âœ… Assistant message persisted (duration: ${persistDuration}ms)`);
           }
         } catch (persistError) {
           console.error(`[${requestId}] âŒ Error persisting assistant message:`, persistError);
         }

         return res.json({
           assistant_message: assistantMessage,
           timing_ms
         });
       } catch (error) {
         const timing_ms = Date.now() - startTime;
         console.error(`[${requestId}] ðŸ’¥ Chat endpoint error after ${timing_ms}ms:`, error);

         return res.status(500).json({
           error: 'Agent processing failed',
           message: config.nodeEnv === 'development' ? (error as Error).message : 'Something went wrong',
           timing_ms
         });
       }
     })().catch((error) => {
       const timing_ms = Date.now() - startTime;
       console.error(`[${requestId}] ðŸ’¥ Unhandled error after ${timing_ms}ms:`, error);

       return res.status(500).json({
         error: 'Internal server error',
         message: 'Something went wrong',
         timing_ms
       });
     });
   });

# CI Workflow Validations

*   **OpenAI API Key Validation**: The CI workflow must validate the OpenAI API key before running tests. This ensures that tests are not run with an invalid or missing key, which can lead to test failures. The validation should include:
    *   Checking if the `OPENAI_API_KEY` environment variable is set.
    *   Checking if the API key starts with `sk-`.
    *   Checking if the API key has a minimum length of 40 characters.
    *   If any of these checks fail, the CI workflow should exit with an error.
*   **CI Dependency Installation**: The CI workflow must use `npm ci` instead of `npm install` to ensure deterministic and clean installs using `package-lock.json`. After installing dependencies, it must verify that critical modules like `ts-jest` are correctly installed.
*   **Jest ts-jest Module Resolution**: Jest in CI environments needs an absolute path to the `ts-jest` transformer. Use `require.resolve('ts-jest')` in the transform configuration to provide absolute path resolution for CI environments. This ensures that Jest can find the `ts-jest` module, as CI environments may have different module resolution than local development.
*   **CI Service Readiness Verification**: Add additional service verification with retry logic in CI to ensure services are fully ready before running tests.
    *   Implement a 5-attempt verification loop with 3-second intervals.
    *   Use explicit HTTP status checks for both frontend (200) and agent (200).
    *   Implement better error reporting when services aren't ready.
*   **CI Test Retry Mechanism**: Implement a test retry mechanism for flaky integration tests.
    *   Enable automatic retry for failed integration tests in CI.
    *   Include a 5-second pause before retry to allow services to stabilize.
    *   Log retry attempts and outcomes clearly.
*   **Jest CI-Specific Configuration**: Configure Jest to be more resilient in CI environments.
    *   Increase the test timeout in CI environments (e.g., 90s vs 60s).
    *   Enable verbose logging in CI for better debugging.
    *   Add Jest retry configuration for CI environments.
        *   Force verbose output in CI to see failed tests.
        *   Show individual test results.
*   **If Supabase environment variables are empty** in E2E tests, ensure that the CI workflow:
    *   Checks if Supabase is running (`supabase status`).
    *   Starts Supabase if it's not running (`supabase start`).
    *   Extracts Supabase credentials from the environment file (`/tmp/supabase.env`).
    *   Sets fallback values for the environment variables if extraction fails.
*   To accurately test error handling for missing `conversation_id` in chat requests, ensure that the error message assertion checks for "Network request failed" OR `"Invalid request"`.

## DEBUGGING

*   The pre-commit hooks should include a comprehensive linting process.
    *   First, run `eslint --fix` to automatically fix fixable issues.
    *   Then, run `eslint` to catch non-fixable errors, such as floating promises.
    *   Include a TypeScript compilation check (`tsc --noEmit`) to catch type errors.
*   When debugging CI pipelines, be aware of process isolation issues. Ensure that background processes started in one CI step are available in subsequent steps.
*   The pre-commit hooks are designed to run on staged files that match specific patterns. Ensure that all relevant files are staged before committing.
*   When troubleshooting integration test failures, consider the test environment and configuration.
    *   Be mindful of the differences between test, development, and production environments.
    *   Ensure that all necessary environment variables are set correctly.
    *   Check for any hardcoded configurations that may conflict with the test environment.
*   When pre-commit hooks fail, it could be due to differences in ESLint configurations, missing rules, or the pre-commit hook scope.
*   To test and validate pre-commit hooks, run linting and TypeScript compilation locally to ensure that all checks pass before committing.
*   It's crucial to have a valid OpenAI API key in CI for integration tests. If the API key is invalid, the agent might run in mock mode and the tests will fail because they expect real responses.
*   When using `expect(async () => { ... }).rejects.toThrow()`, ensure that the promise is awaited. For example, use `await expect(async () => { ... }).rejects.toThrow()`.
*   The root cause of failing integration tests can often be traced back to malformed asynchronous functions. This can lead to uncaught exceptions and unexpected behavior.
    *   Always ensure that try-catch blocks are correctly structured and that asynchronous functions are properly awaited.
*   It's crucial to test in an environment that closely mirrors the CI environment, especially regarding environment variables.
*   When pre-commit hooks fail to catch errors, consider:
    *   ESLint configuration differences
    *   Missing rules
    *   Pre-commit hook scope
    *   That `eslint --fix` only fixes fixable errors, not all ESLint errors
*   When diagnosing CI integration test failures:
    *   Review test logs for specific error messages and stack traces
    *   Check service status and connectivity
    *   Consider process isolation and timing issues
    *   Verify environment variable configurations
    *   Distinguish between test setup issues and application code errors
*   If pre-commit hooks don't catch all errors, consider that the CI environment might have stricter ESLint rules enabled than your local environment, or that the rule might not be enabled in your local ESLint config, or that the rule might not be enabled in your local ESLint config, or that `eslint --fix` only fixes fixable errors, not all ESLint errors.
*   If integration tests fail in CI, and the error is `Preset ts-jest not found relative to rootDir`, this is likely due to:
    *   Missing `ts-jest` in CI dependencies.
    *   A different `node_modules` structure in CI compared to the local environment.
    *   A working directory or path resolution mismatch within the CI environment.
    *   To resolve this:
        *   The `preset` is required, but the explicit `transform` configuration should be present and correctly configured.
        *   **Set `rootDir: "."` in `jest.integration.config.js`** so Jest uses the current working directory.
        *   This bypasses Jest's preset resolution mechanism entirely.
        *   Jest will then use the direct `ts-jest` transformer without trying to resolve a preset, avoiding module resolution issues and maintaining the same TypeScript processing.
*   If integration tests are failing with the message "Module ts-jest in the transform option was not found.", this indicates a problem with module resolution. To resolve this:
    *   The `preset` is required, but the explicit `transform` configuration should be present and correctly configured.
    *   **Set `rootDir: "."` in `jest.integration.config.js`** so Jest uses the current working directory.
    *   This bypasses Jest's preset resolution mechanism entirely.
    *   Jest will then use the direct `ts-jest` transformer, avoiding module resolution issues.
*   If integration tests are failing with the message "Module ts-jest in the transform option was not found.", and setting `rootDir` to `.` does not resolve the issue, and the error persists, it indicates that Jest needs an absolute path to the `ts-jest` transformer in CI. To resolve this:
    *   Replace `'ts-jest'` string with `require.resolve('ts-jest')` in the transform configuration.
    *   This provides absolute path resolution for CI environments.
    *   This ensures that Jest can find the `ts-jest` module, as CI environments may have different module resolution than local development.
*   If E2E tests fail with the error `expect(locator).toBeVisible() failed` for a textarea, consider these potential causes:
    *   The API is returning 500 errors, preventing the chat interface from loading.
    *   An invalid OpenAI API key is being used, resulting in a 401 error.
    *   The chat interface is failing to load for other reasons.
*   When E2E tests fail because the textarea isn't found, check for:
    *   500 errors from the API
    *   Invalid OpenAI API key (401 error)
    *   General chat interface loading failures
*   When debugging E2E tests and the textarea is not rendering, consider the following:
    *   **API Errors**: Conversation/messages endpoints might be returning failures.
    *   **Invalid OpenAI API Key**: The agent service might be returning 401 responses.
    *   **Service Connectivity**: The agent service might not be responding.
    *   **Database Issues**: There might be Supabase connection problems.
*   To further debug E2E tests with textarea visibility issues, follow these debugging steps:
    *   **Add comprehensive debugging to E2E tests**: This should include logging for textarea visibility, API calls, and render state.
    *   **Add detailed logging to the conversation page**: This should include logging for API calls with status codes, environment variable logging, and timeout handling with fallback states.
    *   **Add debug logging to the ChatInterface component**: This should include logging for render state, props, and state change tracking.
    *   **Create a dedicated debug test for textarea visibility issues**.
    *   **Capture Console Errors**: Monitor and log any errors output to the browser console during test execution.
    *   **Monitor Network Errors**: Log any failed network requests (e.g., 500 errors) that might be preventing the textarea from rendering.
    *   **Use Multiple Textarea Detection Strategies**: Try different locators to find the textarea, such as by placeholder text or CSS class.
    *   **Validate Page State**: Check for loading states, error messages, and conversation data to understand if the page is loading correctly.
    *   **Capture Screenshots**: Take screenshots during test execution to visually inspect the page and identify any rendering issues.
*   When debugging Playwright tests, an error such as `locator.all: Unexpected token "=" while parsing css selector ".animate-spin, [class*="loading"], text=Loading". Did you mean to CSS.escape it?` indicates a syntax error in the CSS selector. To resolve this, correct the selector syntax. For example, use separate locators for text-based elements instead of combining them in a single selector.
*   To systematically address module resolution issues in CI environments:
    *   **Use `npm ci` instead of `npm install`**: This ensures deterministic, clean installs using `package-lock.json`.
    *   **Add dependency verification steps**: After installing dependencies, verify that critical modules like `ts-jest` are correctly installed.
    *   **Consider absolute path resolution**: If module resolution continues to fail, use `require.resolve()` to provide absolute paths to modules in configuration files.
*   To improve CI performance, consider parallelizing integration tests by splitting them into separate jobs.
    *   Example: Create separate jobs for agent and frontend integration tests.
    *   Ensure that the E2E job depends on both integration test jobs to avoid running E2E tests before integration tests have passed.
*   To further improve CI resilience for frontend integration tests:
    *   Add additional service verification with retry logic in CI to ensure services are fully ready before running tests.
        *   Implement a 5-attempt verification loop with 3-second intervals.
        *   Use explicit HTTP status checks for both frontend (200) and agent (200).
        *   Implement better error reporting when services aren't ready.
    *   Implement a test retry mechanism for flaky integration tests.
        *   Enable automatic retry for failed integration tests in CI.
        *   Include a 5-second pause before retry to allow services to stabilize.
        *   Log retry attempts and outcomes clearly.
    *   Configure Jest to be more resilient in CI environments.
        *   Increase the test timeout in CI environments (e.g., 90s vs 60s).
        *   Enable verbose logging in CI for better debugging.
        *   Add Jest retry configuration for CI environments.
            *   Force verbose output in CI to see failed tests.
            *   Show individual test results.
*   If one integration test consistently fails in CI but passes locally, consider:
    *   Adding additional service verification with retry logic in CI to ensure services are fully ready before running tests.
        *   Implement a 5-attempt verification loop with 3-second intervals.
        *   Use explicit HTTP status checks for both frontend (200) and agent (200).
        *   Implement better error reporting when services aren't ready.
    *   Implementing a test retry mechanism for flaky integration tests.
        *   Enable automatic retry for failed integration tests in CI.
        *   Include a 5-second pause before retry to allow services to stabilize.
        *   Log retry attempts and outcomes clearly.
    *   Configuring Jest to be more resilient in CI environments.
        *   Increase the test timeout in CI environments (e.g., 90s vs 60s).
        *   Enable verbose logging in CI for better debugging.
        *   Add Jest retry configuration for CI environments.
            *   Force verbose output in CI to see failed tests.
            *   Show individual test results.
*   **Agent Service Integration: Handling Missing conversation_id**: To accurately test error handling for missing `conversation_id` in chat requests, ensure that the error message assertion checks for "Network request failed" OR `"Invalid request"`.
    *   To accurately test error handling for missing `conversation_id` in chat requests, ensure that the error message assertion checks for `"Network request failed"` OR `"Invalid request"`.
*   **If Supabase environment variables are empty** in E2E tests, ensure that the CI workflow:
    *   Checks if Supabase is running (`supabase status`).
    *   Starts Supabase if it's not running (`supabase start`).
    *   Extracts Supabase credentials from the environment file (`/tmp/supabase.env`).
    *   Sets fallback values for the environment variables if extraction fails.

*   **E2E Test Debugging: Handling textarea visibility issues**

    To further debug E2E tests with textarea visibility issues, follow these debugging steps:
    *   **Add comprehensive debugging to E2E tests**: This should include logging for textarea visibility, API calls, and render state.
    *   **Add detailed logging to the conversation page**: This should include logging for API calls with status codes, environment variable logging, and timeout handling with fallback states.
    *   **Add debug logging to the ChatInterface component**: This should include logging for render state, props, and state change tracking.
    *   **Create a dedicated debug test for textarea visibility issues**.
    *   **Capture Console Errors**: Monitor and log any errors output to the browser console during test execution.
    *   **Monitor Network Errors**: Log any failed network requests (e.g., 500 errors) that might be preventing the textarea from rendering.
    *   **Use Multiple Textarea Detection Strategies**: Try different locators to find the textarea, such as by placeholder text or CSS class.
    *   **Validate Page State**: Check for loading states, error messages, and conversation data to understand if the page is loading correctly.
    *   **Capture Screenshots**: Take screenshots during test execution to visually inspect the page and identify any rendering issues.