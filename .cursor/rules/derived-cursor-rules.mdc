---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## HEADERS

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

*   **Environment Workflow and Testing Strategy:** Document the environment workflow, explaining the purpose of each environment (Development, Preview, Staging, and Production), the testing strategy for each, and the overall deployment workflow. Include environment-specific configurations and a cost-effective approach using free tiers.
*   **Staging Setup Guide:** Update the staging setup guide to reflect the separate Vercel project approach instead of using Vercel's built-in environment functionality. This includes architectural changes and step-by-step instructions for creating a separate staging project.
*   **Local Supabase via CLI:** Adopt Supabase CLI for local development database (Docker-based). Add `npm run supabase:start`/`supabase:stop` scripts and docs. Create `frontend/.env.local.example` pointing to local Supabase URL/anon key. Document migrations flow: `npx supabase db reset` for local; `db push` for staging/prod. Document environment mapping: Local (CLI) | Staging (hosted `chef-chopsky-staging`) | Production (hosted `chef-chopsky`). Note: the previous `chef-chopsky-local` hosted project has been repurposed as the staging project to stay within free tier limits. Document local Supabase setup and usage.
    *   **Environment Mapping:**
        *   **Local**: Supabase CLI (Docker) - `http://127.0.0.1:54321`
        *   **Staging**: Hosted Supabase project `chef-chopsky-staging`
        *   **Production**: Hosted Supabase project `chef-chopsky-production`

## TECH STACK

## CODING STANDARDS

## WORKFLOW & RELEASE RULES

*   **Vercel Environment Variables:** When setting environment variables in Vercel projects, the separate staging project approach uses the "production" environment type for the staging project. This is done to stay within free tier limits. This may impact the commands used to sync environment variables.
*   **Syncing Vercel Environment Variables:** The `sync-vercel-env.sh` script can be used to sync environment variables. The separate staging project uses the "production" environment type, so the command should be run with the `--environment production` flag.
    *   When syncing environment variables for the staging environment, first navigate to the frontend directory (`cd frontend`). Then, link the Vercel project (`vercel link --yes --project chef-chopsky-staging`). Finally, run the sync script from the parent directory (`cd ..; ./scripts/sync-vercel-env.sh --file frontend/.env.staging --environment production`).
*   **Feature Branch Staging Deployments**: The trigger for feature branch staging deployments uses the following logic:
    *   **Triggers**: The workflow is automatically triggered when code is pushed to any branch that starts with `feat/` or `feature/`.
    *   **URL Generation**: The deployment URL is created by taking the branch name (e.g., `feat/new-recipe-search`), replacing `/` with `-` (becomes `feat-new-recipe-search`), and creating the URL: `https://chef-chopsky-git-feat-new-recipe-search.vercel.app`.
*   **Pre-Commit Hooks Implementation:** This project uses [Husky](https://typicode.github.com/husky/) and [lint-staged](https://github.com/okonet/lint-staged) to automatically run linting checks before commits.
    *   To setup:
        ```bash
        npm install
        ```
    *   The lint-staged configuration is in `package.json`:
        ```json
        {
          "lint-staged": {
            "frontend/**/*.{js,jsx,ts,tsx}": [
              "cd frontend && npx eslint --fix",
              "cd frontend && npx eslint",
              "cd frontend && npx tsc --noEmit"
            ],
            "agent/src/**/*.{js,ts}": [
              "cd agent && npx eslint --fix",
              "cd agent && npx eslint",
              "cd agent && npx tsc --noEmit"
            ]
          }
        ```
    *   The pre-commit hook script is in `.husky/pre-commit`:
        ```bash
        #!/usr/bin/env sh
        . "$(dirname -- "$0")/_/husky.sh"

        # Run lint-staged to check staged files
        npx lint-staged
        ```
    *   To bypass the pre-commit hooks:
        ```bash
        git commit --no-verify -m "your message"
        ```
        *   **Warning**: Only use this in emergencies! It defeats the purpose of the pre-commit hooks and can lead to CI failures.
    *   If the pre-commit hook isn't running:
        1.  Make sure Husky is installed:
            ```bash
            npm install
            ```
        2.  Verify the hook is executable:
            ```bash
            ls -la .husky/pre-commit
            ```
        3.  Reinstall Husky:
            ```bash
            npm uninstall husky && npm install --save-dev husky
            npx husky
            ```
    *   If you're getting linting errors:
        1.  **Run linting manually** to see the full output:
            ```bash
            npm run lint
            ```
        2.  **Fix issues automatically** where possible:
            ```bash
            cd frontend && npx eslint --fix .
            cd agent && npx eslint src
            ```
        3.  **Fix remaining issues manually** based on the error messages
    *   If the pre-commit hook is slow:
        1.  Only stage the files you want to commit (avoid `git add .` for large changesets)
        2.  The hook only checks staged files, so committing smaller chunks will be faster
        3.  Consider increasing system resources if linting is consistently slow
*   **Git Worktrees Setup**: Use Git worktrees to work on multiple branches simultaneously.
    *   **Directory Structure**: Create a directory structure like this:
        ```
        /Users/daviswhitehead/Documents/chef-chopsky/          # Main repo (current branch)
        /Users/daviswhitehead/Documents/chef-chopsky-worktrees/ # Worktrees directory
        â”œâ”€â”€ main/                                              # Main branch worktree
        â”œâ”€â”€ staging/                                           # Staging branch worktree
        â”œâ”€â”€ feature-branch-1/                                 # Any feature branch
        â””â”€â”€ feature-branch-2/                                 # Another feature branch
        ```
    *   **Commands**:
        ```bash
        # 1. Create worktrees directory
        mkdir -p /Users/daviswhitehead/Documents/chef-chopsky-worktrees

        # 2. Navigate to main repo
        cd /Users/daviswhitehead/Documents/chef-chopsky

        # 3. Create worktrees (adjust branch names as needed)
        git worktree add ../chef-chopsky-worktrees/main main
        git worktree add ../chef-chopsky-worktrees/staging staging

        # 4. Verify setup
        git worktree list

        # 5. Test a worktree
        cd ../chef-chopsky-worktrees/main
        git status
        ```
    *   **Working with Worktrees**:
        *   To work on a specific branch:
            ```bash
            # Navigate to the worktree directory
            cd /Users/daviswhitehead/Documents/chef-chopsky-worktrees/main

            # Work normally - git commands work as expected
            git status
            git add .
            git commit -m "Your changes"
            git push origin main
            ```
        *   To create a new branch in a worktree:
            ```bash
            # From any worktree directory
            git checkout -b new-feature-branch
            git push -u origin new-feature-branch
            ```
    *   **Managing Worktrees**:
        *   Remove a worktree when done:
            ```bash
            # From the main repo directory
            cd /Users/daviswhitehead/Documents/chef-chopsky/

            # Remove worktree (this will delete the directory)
            git worktree remove ../chef-chopsky-worktrees/main
            ```
        *   Prune worktrees (clean up references to deleted directories):
            ```bash
            git worktree prune
            ```
    *   **IDE Setup Considerations**:
        *   For Cursor/VS Code:
            *   Open each worktree as a separate workspace.
            *   Create workspace files for each worktree:
                ```bash
                # In each worktree directory
                code .  # Opens that worktree in VS Code
                ```
    *   **Environment Considerations**:
        *   Each worktree will need its own `.env` files.
        *   Run different services in different worktrees simultaneously (e.g., frontend on port 3000, agent on port 3001 in one worktree, and different ports (3002, 3003) in another worktree).
    *   **Recommended Workflow**:
        1.  **Main repo**: Keep for your current branch.
        2.  **Main worktree**: For stable main branch work.
        3.  **Staging worktree**: For staging environment testing.
        4.  **Feature worktrees**: For new feature development.

## DEBUGGING

*   **Staging vs Production Supabase Credentials:** If staging writes are appearing in production, it's likely that the Vercel environment variables for both staging and production are linked to the same Supabase project. The `NEXT_PUBLIC_SUPABASE_URL` and keys should be different for each environment. To fix this:
    *   In Vercel, for the `chef-chopsky-staging` project, set `NEXT_PUBLIC_SUPABASE_URL` and `NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY` to your STAGING project values and `SUPABASE_SECRET_KEY` to the STAGING service role key.
    *   In Vercel, for the `chef-chopsky` (production) project, ensure those three env vars are production values only. The production Supabase project is actually `chef-chopsky-production`, not `chef-chopsky`.
    *   Redeploy both projects after saving changes.

## AI CODING ASSISTANT RULES

### GENERAL BEHAVIOR
- You are an expert software engineer.
- Follow instructions precisely and completely.
- Ask clarifying questions before proceeding if instructions are ambiguous.
- Generate high-quality, well-documented code.
- Adhere to project standards and best practices.
- Focus on solving the user's problem efficiently and effectively.
- Use the tools available to you to gather information and complete tasks.
- Never expose secrets or API keys in the codebase or in any generated text.
- Use `docs/` to learn about architectural decisions, API usage, and data models.
- Use comments to explain complex code.
- Use descriptive variable names.
- Keep code DRY (Don't Repeat Yourself).
- Acknowledge all steps.

### TOOLS
- You have access to the following tools:
  - `codebase_search`: Search the codebase for relevant information.
  - `file_read`: Read the content of a file.
  - `file_write`: Write content to a file.
  - `file_delete`: Delete a file.
  - `mcp_cursor-playwright_browser_snapshot`: Take a screenshot of the browser.
  - `mcp_cursor-playwright_browser_console_messages`: Returns all console messages.
  - `mcp_cursor-playwright_browser_click`: Perform click on a web page
  - `mcp_cursor-playwright_browser_type`: Type text into editable element
  - `mcp_cursor-playwright_browser_wait_for`: Wait for text to appear or disappear or a specified time to pass
  - `mcp_Railway_get-logs`: Get the logs of a Railway service.
  - `mcp_Railway_list-variables`: List Railway environment variables.
  - `mcp_Railway_check-railway-status`: Check if Railway CLI is installed and user is logged in.
  - `mcp_Railway_set-variables`: Set environment variables for a Railway service.
  - `run_terminal_cmd`: Execute a terminal command.
  - `apply_patch`: Apply a patch to a file.
  - `web_search`: Web search for relevant information (e.g., library documentation, usage examples).
  - `list_dir`: List the files in a directory.
  - `glob_file_search`: Search for files matching a glob pattern.
  - `fetch_rules`: Fetch the active rules.
  - `mcp_vercel_list_teams`: List the user's teams.
  - `mcp_vercel_list_projects`: List all Vercel projects for a user.
  - `mcp_vercel_get_project`: Get a specific project in Vercel.
  - `mcp_vercel_get_deployment`: Get a specific deployment by ID or URL.
  - `mcp_vercel_get_deployment_build_logs`: Get the build logs of a deployment by deployment ID or URL.
  - `mcp_vercel_deploy_to_vercel`: Deploy the current project to Vercel
  - `search_replace`: Performs a find and replace operation.
  - `read_lints`: Reads linting errors in a file.
  - `grep`: Searches for a pattern in a file or directory.
  - `todo_write`: Write a todo list.

### SECURITY
- Never read or write `.env` files directly. This includes `.env.local`, `.env.production`, and `.env.staging`.
- Never output the contents of `.env` files or any sensitive configuration files.
- Instead, use the `codebase_search` tool to find relevant information and the `user` to provide the actual API keys or secrets.
- When creating or modifying code, always use parameterized queries to prevent SQL injection attacks.
- Sanitize user input to prevent XSS attacks.
- When using external libraries, carefully review their documentation and security implications.

### STRATEGIC ASSISTANT PROMPT

Here is a strategic assistant prompt to consider:
```
As an AI assistant, focus on creating a clear and maintainable structure for the project, while making sure that the code meets security and data privacy standards. Be prepared to explain the reasoning behind any architectural decisions, and to suggest alternative approaches when necessary.
```

### CODEBASE SEARCH
- When using `codebase_search`, be specific and descriptive in your search query.
- Focus on finding relevant code snippets, data models, or architectural patterns.
- Do not use `codebase_search` to find secrets or API keys.
- If you can't find something with `codebase_search`, ask the user for more information or context.
- Prefer searching specific files or directories over the entire codebase.

### FILE READING & WRITING
- When using `file_read`, be sure to specify the correct file path.
- When using `file_write`, create a new file if necessary.
- When modifying an existing file, be sure to preserve the original formatting and structure.
- Before writing to a file, confirm that you have a clear understanding of the file's purpose and content.
- If you're asked to write an executable script, be sure to set the correct permissions (e.g., `chmod +x`).

### RUN TERMINAL COMMAND
- Use the `run_terminal_cmd` tool to execute terminal commands.
- Be careful when executing commands that could modify the system or data.
- Always confirm with the user before executing destructive commands (e.g., deleting files, dropping databases).
- Only run commands that are necessary to complete the task.
- Do not run commands that could expose secrets or API keys.
- Always run commands one at a time and do not chain them with `&&` or `||`.
- When running commands, always specify the full absolute path to the command.

### MULTI-TOOL USAGE
- Chain multiple tools together to accomplish complex tasks.
- For example, use `codebase_search` to find a file, then use `file_read` to read its content, and then use `file_write` to modify it.
- Use `list_dir` to explore directory structures before attempting to read or write files.

### EXTERNAL API CALLS
- Only make calls to external APIs when necessary.
- Always use a secure HTTPS connection.
- Never include secrets or API keys in the URL.
- Use the appropriate HTTP method (e.g., `GET`, `POST`, `PUT`, `DELETE`).
- Handle errors gracefully.

### CREATING EXECUTABLE SCRIPTS
- When creating shell scripts, always include a shebang line (`#!/bin/bash`).
- Make the script executable using `chmod +x`.
- Use descriptive variable names.
- Add comments to explain complex logic.
- Test the script thoroughly before using it in production.

### CODE COMMENTS
- Add comments explaining the high-level purpose of functions, classes, and modules.
- Explain complex logic or algorithms in detail.
- Use comments to document the purpose of variables and constants.
- Use comments to explain the expected input and output of functions.
- Use comments to document any known limitations or issues.

### CURSOR RULES
- Adhere to all cursor rules and guidelines.
- Use cursor hooks to enforce project standards and prevent common errors.
- Use cursor snippets to insert code templates.
- Use cursor commands to automate repetitive tasks.
- The `.cursorignore` file blocks `.env` files, but `.env.example` files are allowed with the `!.env.example` rule. To allow creating `.env.local.example`, `.env.staging.example`, and `.env.production.example`, add the following rules to the `.cursorignore` file:
  ```
  !.env.local.example
  !.env.staging.example
  !.env.production.example
  ```

### AI DEVELOPMENT PLAYBOOK
- Follow the AI development playbook for all AI-related tasks.
- Use the appropriate AI models and tools for the task at hand.
- Test AI models thoroughly and validate their performance.
- Document all AI-related code and configurations.

### DOCUMENTATION
- Create documentation for all new code and features.
- Update existing documentation when making changes.
- Use a consistent documentation style.
- Include examples and usage instructions.
- Keep documentation up-to-date.

### CODE CONTRIBUTIONS
- All code contributions should follow the project's coding standards and style guidelines.
- Code should be well-documented and easy to understand.
- All code should be tested thoroughly before being submitted for review.
- Code reviews should be completed in a timely manner.
- All code contributions should be reviewed by at least one other developer before being merged into the main branch.

### ENVIRONMENT MANAGEMENT
- Never hardcode environment-specific values in the codebase.
- Always use environment variables to configure the application.
- Use a consistent naming convention for environment variables.
- Document all environment variables in the `docs/` directory.
- Use separate environment variables for local development, staging, and production.
- Store secrets and API keys securely using a dedicated secrets management system.
- Use a tool like `direnv` to manage environment variables during local development.
- Implement a system for validating environment variables at startup.
- Never store sensitive information in the codebase or in configuration files that are stored in the codebase.
- Follow a 3-environment architecture: Development, Staging, and Production.

### DEPLOYMENT AND INFRASTRUCTURE
- Use Infrastructure-as-Code (IaC) tools to manage infrastructure.
- Automate deployments using CI/CD pipelines.
- Monitor application performance and health using monitoring tools.
- Implement rollback procedures for failed deployments.
- Use a tool like Terraform or CloudFormation to provision infrastructure.
- Automate infrastructure provisioning and configuration.
- Design for scalability and resilience.
- Implement disaster recovery procedures.
- Use a CDN to serve static assets.
- Implement caching to improve performance.

### PRODUCTION SAFETY RULES
- Validate critical configuration on startup.
- Fail loudly and immediately if production requirements are not met.
- Never fall back to mock/test data in production environments.
- Implement environment-specific validation.
- Use different behaviors for development vs production.
- Make production failures impossible to ignore.
- To access your deployed frontend:
  1. Visit the Vercel Dashboard
  2. Navigate to your project
  3. Click on the deployment to access it with proper authentication
  4. Or disable protection if you want public access (in project settings)
- To bypass this for testing:
  1. Log into Vercel in your browser
  2. Access through the dashboard
  3. Or disable protection in project settings if you want public access

### CURSOR RULES
- Adhere to all cursor rules and guidelines.
- Use cursor hooks to enforce project standards and prevent common errors.
- Use cursor snippets to insert code templates.
- Use cursor commands to automate repetitive tasks.
- The `.cursorignore` file blocks `.env` files, but `.env.example` files are allowed with the `!.env.example` rule. To allow creating `.env.local.example`, `.env.staging.example`, and `.env.production.example`, add the following rules to the `.cursorignore` file:
  ```
  !.env.local.example
  !.env.staging.example
  !.env.production.example
  ```

### AI DEVELOPMENT PLAYBOOK
- Follow the AI development playbook for all AI-related tasks.
- Use the appropriate AI models and tools for the task at hand.
- Test AI models thoroughly and validate their performance.
- Document all AI-related code and configurations.

### ENVIRONMENT VALIDATION AND PRODUCTION GUARDS
- The agent service must implement environment validation at startup and production guards.
- The agent service must fail fast in production if required keys are missing or placeholders.
- The agent service must add clear console warnings in non-prod mock modes.
- When the agent service is run in development mode with an invalid API key, it MUST show clear warnings about mock mode with helpful instructions
- The agent service MUST show a warning and helpful instructions for setting the API key when running with an invalid API key in development mode.
- The agent service MUST work normally without warnings with a valid API key.
- The agent service MUST work in production with a valid API key.
- Testing the production safety guards implementation MUST include the following:
    - Missing API Key (Should Fail Fast)
    - Invalid API Key in Production (Should Fail Fast)
    - Placeholder API Key in Production (Should Show Warnings)
    - Invalid API Key in Development (Should Show Warnings)
    - Valid API Key (Should Work Normally)
    - Production with Valid API Key (Should Work)
- The Next.js build process runs in production mode regardless of the `NODE_ENV` environment variable. To allow CI builds to proceed, the production safety check must allow CI builds to proceed by adding a `!process.env.CI` condition.
    - The production safety guard now:
        - Allows CI builds to proceed (CI=true)
        - Still blocks actual production deployments with mock config
        - Maintains security for real production environments

### ENVIRONMENT CONFIGURATION
- When configuring environment-driven retriever and embedding:
    - Read `RETRIEVER_PROVIDER` and `EMBEDDING_MODEL` from environment variables
    - Set environment-specific defaults:
        - Local/Development: `memory` retriever (fast, no external dependencies)
        - Staging: `pinecone` retriever (production-ready)
        - Production: `pinecone` retriever (production-ready vector store)
- The `appEnv` configuration must be updated to properly handle staging.

### ENVIRONMENT DISCRIMINATORS
- The agent service must add environment discriminators to all vector stores, including Elastic and Pinecone filters.

### GITHUB ACTIONS WORKFLOWS
- When creating GitHub Actions workflows, the trigger for feature branch staging deployments uses the following logic:
  - **Triggers**: The workflow is automatically triggered when code is pushed to any branch that starts with `feat/` or `feature/`.
  - **URL Generation**: The deployment URL is created by taking the branch name (e.g., `feat/new-recipe-search`), replacing `/` with `-` (becomes `feat-new-recipe-search`), and creating the URL: `https://chef-chopsky-git-feat-new-recipe-search.vercel.app`.
- After consolidating to Node.js 20.x, the CI workflow should be updated to reflect this change.
- The "test (20.x)" check in `.github/workflows/ci.yml` should be renamed to `Unit Tests & Build (Node 20)` for a more descriptive name.
- The composite action approach should be used to eliminate duplication in GitHub Actions workflows, creating reusable actions to maintain job independence, faster CI, consistency and easy maintenance.
- **GitHub Actions Checkout Step:** When using local actions in GitHub Actions workflows, always include an `actions/checkout` step before running the local action to ensure that the repository code is available.
    - The `actions/checkout` step MUST be included in the `ci.yml` workflow before running the local action.
    - The local action MUST include `actions/setup-node@v4` and `npm run install:all`.
    - The local action should NOT include `actions/checkout@v4` to avoid duplication and conflicts. The `actions/checkout@v4` step must be called in the CI workflow BEFORE the local action.
- **GitHub Actions CI Build Errors**: When the Next.js build fails in the CI environment due to missing Supabase environment variables, the following steps should be taken:
    - Update `frontend/lib/supabase.ts` to replace assertion operators (`!`) with fallback values. Use placeholder values like `https://placeholder.supabase.co` and `placeholder_key` for CI builds.
    - Add CI-specific environment variables to the `.github/workflows/ci.yml` workflow file. These include:
        - `NEXT_PUBLIC_SUPABASE_URL: https://placeholder.supabase.co`
        - `NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY: placeholder_key`
        - `SUPABASE_SECRET_KEY: placeholder_secret`
        - `AGENT_SERVICE_URL: http://localhost:3001`
        - `NEXT_PUBLIC_APP_ENV: ci`
        - `APP_ENV: ci`
    - Ensure that the production safety guards and mock mode detection still work as intended.
- **Disabling GitHub Actions Workflows:** To temporarily disable GitHub Actions workflows, comment out the `on:` section in the workflow file. This prevents the workflow from running on push/PR events while debugging or developing. Remember to re-enable the workflow by uncommenting the `on:` section when finished.
- **Supabase CLI Installation**: The recommended method for installing the Supabase CLI in CI environments is to download the pre-built binary using `curl`, extract it, and move it to `/usr/local/bin/`.
    - Example:
    ```bash
    curl -fsSL https://github.com/supabase/cli/releases/latest/download/supabase_linux_amd64.tar.gz | tar -xz
    sudo mv supabase /usr/local/bin/
    supabase --version
    ```
- **GitHub Actions CI Workflow - Checkout and Local Action:** When using local actions in GitHub Actions workflows, always include an `actions/checkout` step before running the local action to ensure that the repository code is available.
    - The `actions/checkout` step MUST be included in the `ci.yml` workflow before running the local action.
    - The local action MUST include `actions/setup-node@v4` and `npm run install:all`.
    - The local action should NOT include `actions/checkout@v4` to avoid duplication and conflicts. The `actions/checkout@v4` step must be called in the CI workflow BEFORE the local action.
- **GitHub Actions Workflow - Correct Supabase CLI Binary for Linux**: The CI workflow must use the correct Supabase CLI binary for Linux (Ubuntu runners):
    ```bash
    # Install using the recommended method for CI (Linux binary for Ubuntu runners)
    curl -fsSL https://github.com/supabase/cli/releases/latest/download/supabase_linux_amd64.tar.gz | tar -xz
    sudo mv supabase /usr/local/bin/
    ```
- **GitHub Actions Workflow - ESLint No Case Declarations**: When using `const` declarations directly in case blocks without wrapping them in braces, ESLint's `no-case-declarations` rule will be violated, causing the Code Quality job to fail. Fix this by adding braces around the case blocks.
    - Example:
    ```javascript
     switch (provider) {
      case "openai": {
       // Dynamic import to avoid loading LangChain modules before env-loader
       const { OpenAIEmbeddings } = await import("@langchain/openai");
    ```
- **GitHub Actions Workflow - CI Environment Variable:** CI environments must set `CI=true` to allow CI builds to proceed when production safety guards are enabled.
- **GitHub Actions Workflow - Split Integration Tests:** The CI workflow must have separate steps for running frontend and agent integration tests, with the agent tests running first:
    ```yaml
     - name: Run agent integration tests
       run: |
         echo "ðŸ¤– Running agent integration tests..."
         cd agent && npm run test:integration
         
     - name: Run frontend integration tests
       run: |
         echo "ðŸ§ª Running frontend integration tests..."
         cd frontend && npm run test:integration
    ```
- **GitHub Actions Workflow - Consolidate CI Service Orchestration**: To prevent process isolation issues in CI, consolidate service startup, health check, and test execution into a single step.  This ensures that the services started in the CI environment remain available throughout the entire testing process.
  - **Enhanced Debug Logging**:
    - Add detailed environment info logging (Node/NPM versions, ports, working directory)
    - Track service PIDs and exit codes for better debugging
    - Add pre-test service status checks with HTTP response codes
    - Enhanced service startup logging with directory verification
    - Centralized service logs to `/tmp/chef-chopsky-services.log`
    - Detailed test suite results with pass/fail status
- **GitHub Actions Workflow - The pre-commit hook configuration should include running `eslint --fix` first, followed by `eslint` to catch non-fixable errors, ensuring all ESLint errors are caught before committing.** 
- **GitHub Actions Workflow - The pre-commit hook configuration should also include a TypeScript compilation check (`tsc --noEmit`) to catch type errors.**

### RAILWAY CONFIGURATION AND ENVIRONMENT VARIABLES
- **Separate Railway Projects for Environments:** Production and staging environments MUST use separate Railway projects to ensure complete isolation of configurations and data.
- **Staging Environment Variables:** When configuring the staging environment in Railway:
    - `LANGCHAIN_PROJECT` MUST be set to `chef-chopsky-staging`
    - `APP_ENV` MUST be set to `staging`
    - `NODE_ENV` MUST be set to `staging`
    - `LANGCHAIN_INDEX_NAME` MUST be set to `chef-chopsky-staging`
- **Railway Service Names:**
    - The production Railway service is named `chef-chopsky-production`.
    - A separate Railway service MUST be created for staging and named `chef-chopsky-staging`.

### ENVIRONMENT FILES
- When creating or updating environment files, ensure the following files exist and are up to date for both the `@frontend/` and `@agent/` directories:
  - `.env.example`: Core template with essential variables needed across all environments.
  - `.env.local.example`: Local development configuration
  - `.env.staging.example`: Staging environment configuration
  - `.env.production.example`: Production environment configuration

### PINECONE ENVIRONMENT VARIABLE
- The `PINECONE_ENVIRONMENT` variable specifies the environment for your Pinecone project. To find this value:
    1. **Access the Pinecone Console**: Log in to your Pinecone account at [https://app.pinecone.io](https://app.pinecone.io).
    2. **Navigate to the API Keys Section**: In the console, click on the "API Keys" tab on the left-hand side.
    3. **Locate Your Environment**: Here, you'll find your API keys along with the corresponding environment for each key. The environment is typically a string like `us-east-1-aws` or `gcp-starter`.
    4. **Serverless Pinecone Indexes**: If you're using a Serverless Pinecone index, the concept of "environment" has been replaced with "region." In this case, you should use the region value instead of the environment. For more details, refer to Pinecone's [Authentication Documentation](https://docs.pinecone.io/reference/api/authentication). If you're using a Serverless Pinecone index, you might not need `PINECONE_ENVIRONMENT` at all.
    5. **Pod-based Pinecone Indexes**: For older, pod-based Pinecone indexes, you'll need both `PINECONE_API_KEY` and `PINECONE_ENVIRONMENT`. The environment corresponds to the cloud provider and region where your index is hosted.
    6. **Example Values**: Your `.env.production.example` should look like:
        ```bash
        PINECONE_API_KEY=your_production_pinecone_api_key_here
        PINECONE_ENVIRONMENT=us-east-1-aws  # or whatever your environment shows
        ```
    7. **Troubleshooting**:
        - Make sure you're using the latest Pinecone SDK.
        - Check if you're using Serverless indexes (which might not require environment).
        - Contact Pinecone support if you're still having issues.

### CURSOR RULES
- The `.cursorignore` file blocks `.env` files, but `.env.example` files are allowed with the `!.env.example` rule. To allow creating `.env.local.example`, `.env.staging.example`, and `.env.production.example`, add the following rules to the `.cursorignore` file:
  ```
  !.env.local.example
  !.env.staging.example
  !.env.production.example
  ```

### TESTING
- Before merging a pull request, run as many checks as possible locally against the branch and staging. This includes:
    - Running agent tests that are failing in GitHub
    - Running frontend tests (Node 20.x) that are failing
    - Running health checks and environment separation tests
    - Running staging deployment validation
    - Running CI checks including security and integration tests
    - Fixing any issues found during local testing.
- When running agent tests locally, be aware that some tests are designed to trigger production safety guards when running with production configurations but invalid API keys. These "failing" tests are actually verifying that the guards are working correctly.
- **90% of your integration tests should use REAL SERVICES.**
- Mock only:
    - External services (LangSmith, third-party APIs).
    - React components (for unit testing).
    - Services that are intentionally unavailable in the test environment.
- Never mock:
    - Your own database operations
    - Your own API endpoints
    - Your own service communication
    - Core business logic
- **For E2E tests**
  - They are configured to run in **multiple environments** with different configurations:
  1.  LOCAL Environment
      *   File: `playwright.config.ts`
      *   Base URL: `http://localhost:3000`
      *   Agent URL: `http://localhost:3001`
      *   Services: Auto-started via `webServer` configuration
      *   Commands:
          *   `npm run test:e2e` - Run E2E tests locally
          *   `npm run test:e2e:ui` - Run with Playwright UI
          *   `npm run test:e2e:headed` - Run in headed mode
  2.  PRODUCTION Environment
      *   File: `playwright.production.config.ts`
      *   Base URL: `https://chef-chopsky-production.vercel.app`
      *   Agent URL: `https://chef-chopsky-production.up.railway.app`
      *   Services: External (no local servers)
      *   Commands:
          *   `npm run test:e2e:production` - Run production E2E tests
          *