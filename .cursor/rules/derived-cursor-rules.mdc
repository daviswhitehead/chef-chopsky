---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## HEADERS

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

*   **Environment Workflow and Testing Strategy:** Document the environment workflow, explaining the purpose of each environment (Development, Preview, Staging, and Production), the testing strategy for each, and the overall deployment workflow. Include environment-specific configurations and a cost-effective approach using free tiers.
*   **Staging Setup Guide:** Update the staging setup guide to reflect the separate Vercel project approach instead of using Vercel's built-in environment functionality. This includes architectural changes and step-by-step instructions for creating a separate staging project.
*   **Local Supabase via CLI:** Adopt Supabase CLI for local development database (Docker-based). Add `npm run supabase:start`/`supabase:stop` scripts and docs. Create `frontend/.env.local.example` pointing to local Supabase URL/anon key. Document migrations flow: `npx supabase db reset` for local; `db push` for staging/prod. Document environment mapping: Local (CLI) | Staging (hosted `chef-chopsky-staging`) | Production (hosted `chef-chopsky`). Note: the previous `chef-chopsky-local` hosted project has been repurposed as the staging project to stay within free tier limits. Document local Supabase setup and usage.
    *   **Environment Mapping:**
        *   **Local**: Supabase CLI (Docker) - `http://127.0.0.1:54321`
        *   **Staging**: Hosted Supabase project `chef-chopsky-staging`
        *   **Production**: Hosted Supabase project `chef-chopsky-production`

## TECH STACK

## CODING STANDARDS

## WORKFLOW & RELEASE RULES

*   **Vercel Environment Variables:** When setting environment variables in Vercel projects, the separate staging project approach uses the "production" environment type for the staging project. This is done to stay within free tier limits. This may impact the commands used to sync environment variables.
*   **Syncing Vercel Environment Variables:** The `sync-vercel-env.sh` script can be used to sync environment variables. The separate staging project uses the "production" environment type, so the command should be run with the `--environment production` flag.
    *   When syncing environment variables for the staging environment, first navigate to the frontend directory (`cd frontend`). Then, link the Vercel project (`vercel link --yes --project chef-chopsky-staging`). Finally, run the sync script from the parent directory (`cd ..; ./scripts/sync-vercel-env.sh --file frontend/.env.staging --environment production`).
*   **Feature Branch Staging Deployments**: The trigger for feature branch staging deployments uses the following logic:
    *   **Triggers**: The workflow is automatically triggered when code is pushed to any branch that starts with `feat/` or `feature/`.
    *   **URL Generation**: The deployment URL is created by taking the branch name (e.g., `feat/new-recipe-search`), replacing `/` with `-` (becomes `feat-new-recipe-search`), and creating the URL: `https://chef-chopsky-git-feat-new-recipe-search.vercel.app`.
*   **Pre-Commit Hooks Implementation:** This project uses [Husky](https://typicode.github.com/husky/) and [lint-staged](https://github.com/okonet/lint-staged) to automatically run linting checks before commits. The pre-commit hook configuration should include running `eslint --fix` first, followed by `eslint` to catch non-fixable errors, ensuring all ESLint errors are caught before committing. The pre-commit hook configuration should also include a TypeScript compilation check (`tsc --noEmit`) to catch type errors.
    *   To setup:
        ```bash
        npm install
        ```
    *   The lint-staged configuration is in `package.json`:
        ```json
        {
          "lint-staged": {
            "frontend/**/*.{js,jsx,ts,tsx}": [
              "cd frontend && npx eslint --fix",
              "cd frontend && npx eslint",
              "cd frontend && npx tsc --noEmit"
            ],
            "agent/src/**/*.{js,ts}": [
              "cd agent && npx eslint --fix",
              "cd agent && npx eslint",
              "cd agent && npx tsc --noEmit"
            ]
          }
        ```
*   **CI Performance Improvements:**
    *   **Caching:** Implement caching for npm dependencies, Playwright browsers, and Next.js build artifacts to reduce download and build times.
        *   Use `actions/setup-node` cache: `cache: 'npm'` for root, `frontend/`, and `agent/` installs.  Key the cache using `cache-dependency-path` to invalidate the cache when dependencies change.
        *   Cache Playwright browsers: `~/.cache/ms-playwright`. Key the cache using the Playwright version.
        *   Cache Next.js build output (`.next` or `.next-ci`) keyed on lockfile + `next.config.js` + source hash. Avoid caching failed build outputs.
    *   **Avoid Duplicate Builds**: Build once in the "Code Quality" job, then upload the build output. Download it in the "Integration & E2E" job instead of rebuilding.
    *   **Parallelization:** Split integration and E2E tests into parallel jobs to reduce the overall CI time. Create an E2E job to run E2E tests using a matrix strategy to shard tests across workers. Ensure each shard discovers the same test files from the same cwd/config to guarantee a consistent and non-overlapping test split. Use Playwright's `--shard n/m` option to split the test set deterministically across workers.
    *   **Test Scoping:** Implement test scoping to run only essential tests on pull requests and full tests on merges or scheduled builds.
*   **Test Execution and Orchestration**
    *   **Test Scoping**: Run unit and smoke integration tests on PRs; reserve full E2E tests for main/label.
    *   **E2E Test Marking**: Mark heavy/slow tests with `@slow` and exclude them from PRs.
    *   **Playwright WebServer**: Use Playwright's `webServer` to start and stop servers only for E2E tests, eliminating the need for separate scripts.
    *   **Test Splitting**: Shard Playwright tests across 2-3 workers or use `--project` splits to reduce test time.
*   **CI Time Optimization**
    *   **Avoid Duplicate Builds**: Build once in the "Code Quality" job, then upload the build output. Download it in the "Integration & E2E" job instead of rebuilding.
    *   **Supabase Startup**: Start Supabase once per job and tear down only at the job end. Avoid repeated `supabase status` calls; export the env once.
*   **Pre-commit Hooks**:
    *   Run `eslint --fix` first to auto-fix fixable issues.
    *   Then run `eslint` to catch non-fixable errors.
    *   Include a TypeScript compilation check (`tsc --noEmit`) to catch type errors.
*   **Next.js Build Process**:
    *   To address the Next.js build process and avoid the `Html should not be imported outside of pages/_document` error, the following configurations must be considered:
        *   **NODE_ENV Consistency**: Use `NODE_ENV=production` specifically for the build step.
        *   **Dynamic Rendering**: Apply `export const dynamic = 'force-dynamic'` to error pages to avoid static generation.
        *   **Separate Build Directory**: Use a dedicated build directory (e.g., `.next-ci`) for CI builds to prevent conflicts.
        Custom Error Pages**: Removing custom error pages (`error.tsx` and `not-found.tsx`) may be necessary, allowing Next.js to handle them with its built-in functionality.
*   **Harden frontend chat API**: guard fetch, safe logging, single retry on transient errors
    *   When performing a fetch to the agent service, ensure the fetch is guarded with a try/catch, and the code performs safe logging.
    *   Implement a single retry on transient errors like AbortError or ECONNREFUSED.
        ```typescript
         // Perform the agent fetch with a single retry on transient errors
         async function fetchAgentOnce(): Promise<Response> {
           return await fetch(`${AGENT_SERVICE_URL}/chat`, {
             method: 'POST',
             headers: { 'Content-Type': 'application/json' },
             body: JSON.stringify({
               conversation_id: conversationId,
               messages: filtered,
               client_metadata: {
                 user_id: userId ?? null,
                 source: 'frontend',
                 user_agent: request.headers.get('user-agent') || 'unknown'
               }
             }),
             // Use centralized timeout configuration; do not assume it's always set
             signal: AbortSignal.timeout(getEnvironmentTimeouts().API_ROUTE || 15000)
           })
         }

         let agentResponse: Response | undefined
         try {
           agentResponse = await fetchAgentOnce()
         } catch (err: any) {
           // Retry once on common transient errors
           const transient = err?.name === 'AbortError' || err?.code === 'ECONNREFUSED' || err?.code === 'ENOTFOUND' || err?.message?.includes('fetch failed')
           if (transient) {
             // small backoff
             await new Promise(r => setTimeout(r, 200))
             try {
               agentResponse = await fetchAgentOnce()
             } catch (err2) {
               const agentDurationErr = Date.now() - agentStartTime;
               console.error(`[${requestId}] ðŸ’¥ Agent fetch failed after retry in ${agentDurationErr}ms:`, err2)
             }
           } else {
             const agentDurationErr = Date.now() - agentStartTime;
             console.error(`[${requestId}] ðŸ’¥ Agent fetch failed (non-transient) in ${agentDurationErr}ms:`, err)
           }
         }
         if (agentResponse) {
           console.log(`[${requestId}] â±ï¸ Agent service responded in ${agentResponse.status}`);
         } else {
           console.error(`[${requestId}] ðŸ’¥ Agent service did not return a response after ${agentDuration}ms`)
           return NextResponse.json(
             { error: 'Agent unavailable', message: 'Failed to reach agent service' },
             { status: 502 }
           )
         }

         if (!agentResponse.ok) {
           const err = await safeJson(agentResponse)
           console.log(`[${requestId}] âŒ Agent service error:`, err);

           // Persist error message to Supabase
           console.log(`[${requestId}] ðŸ’¾ Persisting error message to Supabase`);
           try {
             const errorPersistStart = Date.now();
             const persistResponse = await supabaseClient
               .from('conversation_messages')
               .insert([
                 {
                   conversation_id: conversationId,
                   role: 'system',
                   content: `Agent service error: ${err?.message || 'Unknown error'}`,
                   user_id: userId,
                   source: 'system',
                   created_at: new Date(),
                   message_id: `error-${messageId}` // Unique ID for the error message
                 }
               ]);
             if (persistResponse.error) {
               console.error(`[${requestId}] âŒ Supabase persist error:`, persistResponse.error);
             } else {
               const persistDuration = Date.now() - errorPersistStart;
               console.log(`[${requestId}] âœ… Error message persisted (duration: ${persistDuration}ms)`);
             }
           } catch (persistError) {
             console.error(`[${requestId}] âŒ Error persisting assistant message:`, persistError);
           }
           return NextResponse.json({
             error: 'Agent processing failed',
             message: err?.message || 'Something went wrong',
             timing_ms
           }, { status: 500 });
         }
         const data = await agentResponse.json();
         console.log(`[${requestId}] ðŸ¤– Agent service returned data:`, data);

         const assistantMessage = data.assistant_message;

         if (!assistantMessage) {
           console.error(`[${requestId}] âŒ Agent service returned invalid data:`, data);
           return res.status(500).json({
             error: 'Agent processing failed',
             message: 'Invalid agent response',
             timing_ms
           });
         }

         // Persist agent message to Supabase
         console.log(`[${requestId}] ðŸ’¾ Persisting assistant message to Supabase`);
         try {
           const persistStart = Date.now();
           const persistResponse = await supabaseClient
             .from('conversation_messages')
             .insert([
               {
                 conversation_id: ConversationId,
                 role: assistantMessage.role,
                 content: assistantMessage.content,
                 user_id: userId,
                 created_at: new Date(assistantMessage.created_at ?? Date.now()),
                 message_id: assistantMessage.id,
                 source: 'agent',
                 usage: assistantMessage.usage,
                 model: assistantMessage.model,
               }
             ]);

           if (persistResponse.error) {
             console.error(`[${requestId}] âŒ Supabase persist error:`, persistResponse.error);
           } else {
             const persistDuration = Date.now() - persistStart;
             console.log(`[${requestId}] âœ… Assistant message persisted (duration: ${persistDuration}ms)`);
           }
         } catch (persistError) {
           console.error(`[${requestId}] âŒ Error persisting assistant message:`, persistError);
         }

         return res.json({
           assistant_message: assistantMessage,
           timing_ms
         });
       } catch (error) {
         const timing_ms = Date.now() - startTime;
         console.error(`[${requestId}] ðŸ’¥ Chat endpoint error after ${timing_ms}ms:`, error);

         return res.status(500).json({
           error: 'Agent processing failed',
           message: config.nodeEnv === 'development' ? (error as Error).message : 'Something went wrong',
           timing_ms
         });
       }
     })().catch((error) => {
       const timing_ms = Date.now() - startTime;
       console.error(`[${requestId}] ðŸ’¥ Unhandled error after ${timing_ms}ms:`, error);

       return res.status(500).json({
         error: 'Internal server error',
         message: 'Something went wrong',
         timing_ms
       });
     });
   });

# CI Workflow Validations

*   **OpenAI API Key Validation**: The CI workflow must validate the OpenAI API key before running tests. This ensures that tests are not run with an invalid or missing key, which can lead to test failures. The validation should include:
    *   Checking if the `OPENAI_API_KEY` environment variable is set.
    *   Checking if the API key starts with `sk-`.
    *   Checking if the API key has a minimum length of 40 characters.
    *   If any of these checks fail, the CI workflow should exit with an error.
*   **CI Dependency Installation**: The CI workflow must use `npm ci` instead of `npm install` to ensure deterministic and clean installs using `package-lock.json`. After installing dependencies, it must verify that critical modules like `ts-jest` are correctly installed.
*   **Jest ts-jest Module Resolution**: Jest in CI environments needs an absolute path to the `ts-jest` transformer. Use `require.resolve('ts-jest')` in the transform configuration to provide absolute path resolution for CI environments. This ensures that Jest can find the `ts-jest` module, as CI environments may have different module resolution than local development.
*   **CI Service Readiness Verification**: Add additional service verification with retry logic in CI to ensure services are fully ready before running tests.
    *   Implement a 5-attempt verification loop with 3-second intervals.
    *   Use explicit HTTP status checks for both frontend (200) and agent (200).
    *   Implement better error reporting when services aren't ready.
*   **CI Test Retry Mechanism**: Implement a test retry mechanism for flaky integration tests.
    *   Enable automatic retry for failed integration tests in CI.
    *   Include a 5-second pause before retry to allow services to stabilize.
    *   Log retry attempts and outcomes clearly.
*   **Jest CI-Specific Configuration**: Configure Jest to be more resilient in CI environments.
    *   Increase the test timeout in CI environments (e.g., 90s vs 60s).
    *   Enable verbose logging in CI for better debugging.
    *   Add Jest retry configuration for CI environments.
        *   Force verbose output in CI to see failed tests.
        *   Show individual test results.
*   **If Supabase environment variables are empty** in E2E tests, ensure that the CI workflow:
    *   Checks if Supabase is running (`supabase status`).
    *   Starts Supabase if it's not running (`supabase start`).
    *   Extracts Supabase credentials from the environment file (`/tmp/supabase.env`).
    *   Sets fallback values for the environment variables if extraction fails.
*   To accurately test error handling for missing `conversation_id` in chat requests, ensure that the error message assertion checks for "Network request failed" OR `"Invalid request"`.
*   **Local E2E Test Environment Setup**: Mirror the CI environment locally using Docker Compose for Supabase and other services. Ensure the same environment variables and scripts are used. Create and use `npm run test:e2e:local` script.
*   **E2E Test Debugging Enhancements**:
    *   **Screenshots on Failure**: Configure Playwright to automatically capture screenshots when tests fail.
    *   **Video Recording**: Record test runs for complex failures.
    *   **Detailed Logging**: Increase the verbosity of console output during test runs.
    *   **Test Isolation**: Use `--grep` to run individual tests locally for focused debugging.
    *   **Headful Mode**: Run tests in headful mode (with a visible browser) using `--headed` for visual debugging.
*   **E2E Test Parallelization and Sharding**: Improve test execution time by implementing local sharding and categorizing tests as fast or slow. Consider selective testing to run only changed test files.
*   **Mock/Stub Strategy for E2E Tests**: Use deterministic AI responses for UI tests and mock Supabase with local SQLite for faster tests. Implement service stubs for external dependencies.
*   **Improved CI Feedback Mechanisms**:
    *   **Fail Fast**: Configure CI to stop on the first test failure.
    *   **Structured Error Reporting**: Implement structured error output for better analysis.
    *   **Test Result Artifacts**: Upload detailed test results as artifacts.
    *   **Notification System**: Integrate Slack/Discord alerts for test failures.

## Local CI/CD with Act

*   **Goal**: Replicate the CI environment locally using Docker and Act to enable faster feedback loops for E2E test debugging.
    *   **Installation**:
        ```bash
        # macOS
        brew install act

        # Or via curl
        curl https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash
        ```
    *   **Configuration**:
        *   Create `.github/testing/.actrc` in the project root:
            ```bash
            # Act configuration for local CI execution
            # Use ubuntu-latest runner
            -P ubuntu-latest=catthehacker/ubuntu:act-latest
            # Enable verbose logging for cursor agent access
            -v
            # Use existing environment file
            --env-file .github/testing/env.ci
            # Fix Apple M-series chip compatibility
            --container-architecture linux/amd64
            ```
        *   Act uses the existing `.github/testing/env.ci` file for environment variables.
            *   This file combines both frontend and agent environment variables for CI testing
            *   Used by Act to run GitHub Actions workflows locally
        *   The `.github/testing/env.ci` file should contain:
            *   `OPENAI_API_KEY`: Your OpenAI API key
            *   `LANGCHAIN_API_KEY`: Your LangChain API key (optional)
            *   `NEXT_PUBLIC_SUPABASE_URL`: Local Supabase URL
            *   `SUPABASE_SECRET_KEY`: Local Supabase secret key
            *   Other required environment variables for both services
    *   **Direct E2E Runner Script**:
        *   Create `scripts/run-e2e-local.sh`:
            ```bash
            #!/bin/bash
            # Run E2E tests directly with Act (skip dependencies)
            # This allows immediate E2E testing without waiting for code-quality and integration tests

            set -e

            echo "ðŸš€ Running E2E tests locally with Act..."
            echo "ðŸ“‹ This will run the E2E job directly, skipping code-quality and integration tests"
            echo ""

            # Run E2E tests with Act
            # -W: Specify workflow file to avoid other workflow issues
            # -j e2e: Run only the e2e job
            # --matrix shard:1/1: Run single shard for faster execution
            # -v: Verbose logging for cursor agent access
            # --env-file: Use existing .github/testing/env.ci file
            # --container-architecture: Fix Apple M-series chip compatibility
            act -W .github/workflows/ci.yml -j e2e --matrix shard:1/1 -v --env-file .github/testing/env.ci --container-architecture linux/amd64

            echo ""
            echo "âœ… E2E tests completed!"
            ```
        *   Create `scripts/run-e2e-direct.sh`:
            ```bash
            #!/bin/bash

            # Run E2E tests directly, bypassing dependencies
            echo "ðŸš€ Running E2E tests directly (bypassing dependencies)..."
            echo "ðŸ“‹ This will run only the E2E job without dependencies"
            echo ""

            # Clean up any existing processes first
            echo "ðŸ§¹ Cleaning up any existing processes..."
            pkill -f 'next dev' || true
            pkill -f 'tsx.*server' || true
            pkill -f 'npm run dev' || true
            pkill -f 'npm run server' || true
            sleep 3

            # Run E2E job using E2E-only workflow (no dependencies)
            echo "ðŸŽ¯ Running E2E job using E2E-only workflow..."
            act -W .github/workflows/ci-e2e-only.yml -j e2e --matrix shard:1/1 -v --env-file .github/testing/env.ci --container-architecture linux/amd64

            echo ""
            echo "âœ… E2E tests completed!"
            ```
        *   Make the script executable: `chmod +x scripts/run-e2e-local.sh` and `chmod +x scripts/run-e2e-direct.sh`
    *   **Usage**:
        ```bash
        # Run all jobs (full CI)
        act -W .github/workflows/ci.yml

        # Run specific job
        act -W .github/workflows/ci.yml -j code-quality
        act -W .github/workflows/ci.yml -j integration-tests-agent
        act -W .github/workflows/ci.yml -j integration-tests-frontend

        # Run just E2E tests
        ./scripts/run-e2e-local.sh

        # Run just E2E tests (fastest feedback)
        ./scripts/run-e2e-direct.sh

        # Run specific E2E shard
        act -W .github/workflows/ci.yml -j e2e --matrix shard:1/1

        # Run with verbose logging
        act -W .github/workflows/ci.yml -j e2e -v
        ```
    *   **Key Benefits**:
        *   **No CI changes**: Uses existing `.github/workflows/ci.yml` or `ci-e2e-only.yml`
        *   **Direct E2E execution**: Skip dependencies with `-j e2e`
        *   **Terminal logs**: `-v` flag shows all output
        *   **Local secrets**: Uses `.github/testing/env.ci` file
        *   **Fast iteration**: No push/pull cycle
    *   **Expected Timeline**:
        *   **Setup**: ~15 minutes total
        *   **First run**: ~5-10 minutes (Docker image pull)
        *   **Subsequent runs**: ~2-3 minutes
        *   **Feedback loop**: Seconds instead of minutes

## DEBUGGING

*   The pre-commit hooks should include a comprehensive linting process.
    *   First, run `eslint --fix` to automatically fix fixable issues.
    *   Then, run `eslint` to catch non-fixable errors, such as floating promises.
    *   Include a TypeScript compilation check (`tsc --noEmit`) to catch type errors.
*   When debugging CI pipelines, be aware of process isolation issues. Ensure that background processes started in one CI step are available in subsequent steps.
*   The pre-commit hooks are designed to run on staged files that match specific patterns. Ensure that all relevant files are staged before committing.
*   When troubleshooting integration test failures, consider the test environment and configuration.
    *   Be mindful of the differences between test, development, and production environments.
    *   Ensure that all necessary environment variables are set correctly.
    *   Check for any hardcoded configurations that may conflict with the test environment.
*   When pre-commit hooks fail, it could be due to differences in ESLint configurations, missing rules, or the pre-commit hook scope.
*   To test and validate pre-commit hooks, run linting and TypeScript compilation locally to ensure that all checks pass before committing.
*   It's crucial to have a valid OpenAI API key in CI for integration tests. If the API key is invalid, the agent might run in mock mode and the tests will fail because they expect real responses.
*   When using `expect(async () => { ... }).rejects.toThrow()`, ensure that the promise is awaited. For example, use `await expect(async () => { ... }).rejects.toThrow()`.
*   The root cause of failing integration tests can often be traced back to malformed asynchronous functions. This can lead to uncaught exceptions and unexpected behavior.
    *   Always ensure that try-catch blocks are correctly structured and that asynchronous functions are properly awaited.
*   It's crucial to test in an environment that closely mirrors the CI environment, especially regarding environment variables.
*   When pre-commit hooks fail to catch errors, consider:
    *   ESLint configuration differences
    *   Missing rules
    *   Pre-commit hook scope
    *   That `eslint --fix` only fixes fixable errors, not all ESLint errors
*   If integration tests fail in CI, and the error is `Preset ts-jest not found relative to rootDir`, this is likely due to:
    *   Missing `ts-jest` in CI dependencies.
    *   A different `node_modules` structure in CI compared to the local environment.
    *   A working directory or path resolution mismatch within the CI environment.
    *   To resolve this:
        *   The `preset` is required, but the explicit `transform` configuration should be present and correctly configured.
        *   **Set `rootDir: "."` in `jest.integration.config.js`** so Jest uses the current working directory.
        *   This bypasses Jest's preset resolution mechanism entirely.
        *   Jest will then use the direct `ts-jest` transformer without trying to resolve a preset, avoiding module resolution issues and maintaining the same TypeScript processing.
*   If integration tests are failing with the message "Module ts-jest in the transform option was not found.", this indicates a problem with module resolution. To resolve this:
    *   The `preset` is required, but the explicit `transform` configuration should be present and correctly configured.
    *   **Set `rootDir: "."` in `jest.integration.config.js`** so Jest uses the current working directory.
    *   This bypasses Jest's preset resolution mechanism entirely.
    *   Jest will then use the direct `ts-jest` transformer, avoiding module resolution issues.
*   If integration tests are failing with the message "Module ts-jest in the transform option was not found.", and setting `rootDir` to `.` does not resolve the issue, and the error persists, it indicates that Jest needs an absolute path to the `ts-jest` transformer in CI. To resolve this:
    *   Replace `'ts-jest'` string with `require.resolve('ts-jest')` in the transform configuration.
    *   This provides absolute path resolution for CI environments.
    *   This ensures that Jest can find the `ts-jest` module, as CI environments may have different module resolution than local development.
*   If E2E tests fail with the error `expect(locator).toBeVisible() failed` for a textarea, consider these potential causes:
    *   The API is returning 500 errors, preventing the chat interface from loading.
    *   An invalid OpenAI API key is being used, resulting in a 401 error.
    *   The chat interface is failing to load for other reasons.
*   When E2E tests fail because the textarea isn't found, check for:
    *   500 errors from the API
    *   Invalid OpenAI API key (401 error)
    *   General chat interface loading failures
*   When debugging E2E tests and the textarea is not rendering, consider the following:
    *   **API Errors**: Conversation/messages endpoints might be returning failures.
*   **E2E Test Debugging: Handling textarea visibility issues**

    To further debug E2E tests with textarea visibility issues, follow these debugging steps:
    *   **Add comprehensive debugging to E2E tests**: This should include logging for textarea visibility, API calls, and render state.
    *   **Add detailed logging to the conversation page**: This should include logging for API calls with status codes, environment variable logging, and timeout handling with fallback states.
    *   **Add debug logging to the ChatInterface component**: This should include logging for render state, props, and state change tracking.
    *   **Create a dedicated debug test for textarea visibility issues**.
    *   **Capture Console Errors**: Monitor and log any errors output to the browser console during test execution.
    *   **Monitor Network Errors**: Log any failed network requests (e.g., 500 errors) that might be preventing the textarea from rendering.
    *   **Use Multiple Textarea Detection Strategies**: Try different locators to find the textarea, such as by placeholder text or CSS class.
    *   **Validate Page State**: Check for loading states, error messages, and conversation data to understand if the page is loading correctly.
    *   **Capture Screenshots**: Take screenshots during test execution to visually inspect the page and identify any rendering issues.
*   **Playwright Console Logging**: Playwright suppresses console logs by default. Use the `--reporter=line` flag to enable console output in CI.
*   **E2E Test Structure**: Favour creating conversations first, and navigating to them. Avoid navigating to `/conversations/new` or `/conversations/${randomId}` directly. Instead, use `testEnv.createTestConversation()` to create a conversation via API, then navigate to `/conversations/${conversationId}` with the real ID.
*   **Test Locally**: Make sure any tests you modify pass locally before pushing to remote.
*   **Fix E2E tests to remove non-deterministic AI response dependencies**: Update tests to use deterministic UI signals.
*   **E2E Test Timing Issues:** To prevent race conditions in E2E tests due to timing issues, consider the following:
    *   **Stable Test Hooks**: Add a stable test hook (e.g., `data-testid="message-user"`) to the message bubble component (user message). In the test, wait for the element with the exact text inside that container:
        ```ts
        const userBubble = page.locator('[data-testid="message-user"]').filter({ hasText: TEST_SCENARIOS.SIMPLE_MESSAGE }).first();
        await expect(userBubble).toBeVisible();
        ```
        This removes the race on when Tailwind classes apply.
    *   **Deterministic Events**: Gate on a deterministic event before asserting DOM styles. After sending, wait for the persistence/network to complete, then assert UI:
        ```ts
        // Example if your app posts to /api/conversations/:id/messages
        await page.waitForResponse(res =>
          /\/api\/conversations\/.+\/messages$/.test(res.url()) && res.ok()
        );
        // Then wait for the user bubble hook:
        await expect(page.locator('[data-testid="message-user"]').first()).toBeVisible();
        ```
        Alternatively, wait for DB echo via a GET messages call:
        ```ts
        await page.waitForResponse(res =>
          /\/api\/conversations\/.+\/messages$/.test(res.url()) && res.status() === 200
        );
        ```
    *   **Assert Structure Near Text**: If changing UI isnâ€™t desired, assert structure near the text, not color class. Find the element containing the text and assert its rounded container ancestor:
        ```ts
        const msgText = page.locator(`text=${TEST_SCENARIOS.SIMPLE_MESSAGE}`).first();
        await expect(msgText).toBeVisible();
        const bubble = msgText.locator('xpath=ancestor::*[contains(@class, "rounded-lg")][1]');
        await expect(bubble).toBeVisible();
        ```
        Optional: poll until the bubbleâ€™s class includes bg-chef-500 (only if you really need color):
        ```ts
        await expect.poll(async () => {
          const cls = await bubble.getAttribute('class');
          return cls?.includes('bg-chef-500');
        }, { timeout: 5000 }).toBe(true);
        ```
    *   **Post-Render Settle**: Add a tiny post-render settle before style assertions (last resort). Keep the text/bubble visibility wait, then:
        ```ts
        await page.waitForTimeout(200); // allow hydration/style application
        ```
    *   **Robust Helper**: Increase test robustness with a helper. Replace direct waits with a helper that sequences: wait for text â†’ wait for container â†’ optional style poll â†’ then return the bubble.

This approach eliminates the race by:
* Waiting on a stable, semantic hook (`data-testid`) rather than non-deterministic style timing.
* Synchronizing with network/persistence completion.
* Falling back to structure-based assertions when styles lag.
* Fixed E2E tests to create conversations first.
* E2E tests now auto-create conversation on 404.
* E2E tests no longer rely on non-deterministic AI responses.
* Added more robust Supabase credential extraction with fallback to default local keys.
* E2E test runs now verify that Supabase credentials are set before running tests.
* E2E tests now handle network errors during long requests.
* Added additional service verification with retry logic.
* Added a test retry mechanism for flaky integration
*   **Debugging Act failures in local tests**: When debugging integration-tests-agent failures locally with Act, consider the following:
    *   **Verify the .github/testing/.env.ci file**: Ensure this file exists and contains all required environment variables (OPENAI_API_KEY, LANGCHAIN_API_KEY, etc.).
    *   **Check the specific error**: Identify the exact error message or failure occurring during the Act run. Is it a timeout error, service startup failure, environment variable issue, OpenAI API key validation failure, or Supabase connection problem?
    *   **Determine the failure step**: