---
description: "Generalized AI-first feature development process for shipping new features consistently (agnostic to domain)"
globs:
  - "**/*"
alwaysApply: false
---

# 🧭 AI Feature Development: End-to-End Workflow (Integration Rules)

## 🎯 Purpose
A zoomed-out, repeatable workflow that organizes AI-assisted development from idea to shipped feature, while preserving concrete best practices.

**Applies to**: Any feature (UI, agent/graph, retrieval, API, infra), across web/mobile/backends.

## 📋 The 5-Phase Process

### 1) PLAN (Research → Feature Plan)
- **Discovery**: Use AI chat to explore problem/market, constraints, risks. Save key notes in `documentation/projects/<vN: feature-name>/`.
- **Feature plan (must-have)**:
  - Problem statement and user value
  - Primary user story + 3–5 acceptance criteria
  - Constraints (performance, platform, data, a11y, privacy)
  - Telemetry plan (LangSmith, analytics, logs)
  - Out-of-scope/not-now
- **🗂️ Template**: Use `docs/playbook/templates/intent-template.md` for structured intent capture with AI facilitation.
- **Review**: sanity-check with AI; clarify unknowns; confirm the vertical slice to ship first.

### 2) TASKS (Project Breakdown → Story-Organized Todo List)
- **Branch**: `feat/<short-name>`; use conventional commits.
- **Work plan (must-have)**: Story-oriented task list with subtasks mapping to acceptance criteria.
  - Mark dependencies, mocks vs real integrations, and risks.
- **🗂️ Templates**: Use `docs/playbook/templates/roadmap-template.md` and `docs/playbook/templates/task-breakdown-template.md` for structured planning with AI collaboration.
- **Scaffolding**: Prefer templates and stubs to get to runnable fast.
- After major task(s), prove to me that the task(s) is complete.

### 3) DELIVERY (Build the Vertical Slice → Iterate)
- **Skeleton first, then fill in**: ship a runnable end-to-end path early.
  - Prefer in-memory/mocked providers first; gate external providers (DBs/vector stores/LLMs) behind a `provider`/`driver` config with safe defaults for local dev.
- **Configuration & Secrets**:
  - Keep `.env.example` current.
  - Centralize config (e.g., `agent/src/config/index.ts`), validate required keys.
  - Never leak secrets; use repository secrets in CI.
  - **🚨 CRITICAL**: Validate environment at startup; warn if running in mock mode.
  - **🚨 CRITICAL**: Never ship services with placeholder API keys (`'test-key'`, `'your_api_key_here'`).
  - **🚨 CRITICAL**: Provide automated environment setup scripts (`npm run setup`).
  - **🚨 CRITICAL**: Fail fast in production if critical configuration is missing.
- **Observability by default** (if LLMs):
  - LangSmith: set `project_name`, dynamic `run_name`, `tags`, `metadata` per run/test.
  - Minimal structured logs for key decisions/errors.
  - If streaming APIs are used, ensure callers drain streams or explicitly cancel (AbortSignal/`.return()`); do not break iterators early in tests.
- **Frontend integration (if applicable)**:
  - Reuse patterns/components; RN/RNW + NativeWind + gluestack UI; a11y.
  - Hide incomplete features behind flags/guards.
- **Retrieval/Data (if applicable)**:
  - Start with in-memory implementations by default for local dev and tests.
  - Expose a single `provider`/`driver` switch with env-based configuration for production stores.
  - Provide an ingestion (indexer) path if retrieval is required.
- **Deployment**:
  - Favor low-friction deploys first; document required env.
  - Provide a simple health probe and rollback plan.

### 4) VALIDATION (Test → CI → Ship → Measure)
- **Tests (must-have)**:
  - Jest tests per story: happy path, error handling, timing sanity bound.
  - Fast by default; prefer in-memory/mocks.
  - For streaming clients, drain async iterators (or call `.return()`/abort signals) to avoid open-handle leaks in CI.
- **CI rules**:
  - Run tests on PRs to `main` and on pushes to `main`.
  - Start local services in background with health or port checks.
  - Require only the minimal secrets needed for the default provider (e.g., a single LLM key); external providers optional.
  - Output only terminal logs.
- **LangSmith/Telemetry**:
  - Confirm traces show dynamic run names, tags, and metadata.
  - Capture minimal KPIs defined in the plan.
- **Release**:
  - Merge when CI green and acceptance criteria met.
  - Announce change and update docs.

### 5) CONTINUOUS IMPROVEMENT
- **Reflection & Templates**:
  - Use `docs/playbook/templates/phase-execution-template.md` to track improvements and lessons learned.
  - Document what worked well and what didn't in each phase.
  - Update templates based on experience and findings.
- **Cursor Rules**:
  - Reflect on feature development and update cursor rules in ways that generalize to and streamline future feature development.
  - Apply lessons learned to phase-specific rules (101-105) and core rules (004).
- **Documentation**:
  - Reflect on feature development and update general or feature-specific docs with the most accurate and up-to-date information.
  - Share improvements with team and broader community.
- **Process Enhancement**:
  - Identify bottlenecks and inefficiencies in the current workflow.
  - Propose and test improvements to templates, checklists, and quality gates.

## 📋 Templates & Checklists

### Templates (recommended)
- Scaffold for feature/agent with code + test + docs.

### PR Checklist
- [ ] Feature plan and `.env.example` updated
- [ ] Tests added and passing locally
- [ ] CI green
- [ ] LangSmith traces correct (if LLMs)
- [ ] Frontend guarded if partial
- [ ] **🚨 Environment validation**: Services start without mock mode warnings
- [ ] **🚨 Configuration check**: All API keys are real, not placeholders
- [ ] **🚨 Setup validation**: `npm run setup` passes without errors

## 🎯 Definition of Done
- End-to-end demoable; acceptance criteria met.
- Tests pass locally and in CI.
- Env documented; secrets configured.
- Telemetry visible; rollback noted.

---

*This rule provides a structured approach to AI-assisted feature development.*