## Frontend ↔ Agent Integration (v4)

### Problem & User Value
Users need a seamless, ChatGPT-like chat in Chef Chopsky. Messages from the web UI should be processed by our LangChain/LangGraph agent and responses shown back in the UI. This provides an end-to-end usable experience to validate the core value.

### Primary User Story
As a user, I can start a new conversation and send a message in the web UI, and I receive a helpful assistant reply generated by the Chef Chopsky agent.

### Acceptance Criteria (v1)
1) Create conversation: User can create a conversation from the UI; it persists in Supabase.
2) Send message: Sending a message triggers the agent; both user and assistant messages persist and display in order.
3) Reasonable UX: Minimal loading indicator while the agent responds; final text appears (buffered, no token streaming in v1).
4) Telemetry: Each request is traced with LangSmith (project: "chef chopsky") including basic metadata: conversation_id, message_id(s), environment, and model.
5) Anonymous OK: Works without authentication; later we can attach to Supabase auth.

Out-of-scope for v1
- Token-by-token streaming to UI (we may add in v1.1).
- "Stop generating" control.
- Per-user rate limiting.
- Long-term memory and personalization.

### Constraints
- Keep it simple: prioritize a shippable vertical slice.
- Frontend: Next.js (App Router) using existing components (`ChatInterface`, `ConversationList`, etc.).
- Backend agent: LangChain/LangGraph, Node runtime.
- Persistence: Supabase (existing conversations/messages). We may add light columns for observability later.
- Anonymous usage; avoid auth coupling.

### Architecture Decision (v1)
Integration pattern: Separate Agent Service over HTTP, with a thin proxy in Next.js.

**Why separate services?**
- **Avoids bundling issues:** LangChain/LangGraph are heavy Node.js libraries that don't work in web browsers or Next.js Edge runtime
- **Best-in-class pattern:** ChatGPT, Discord, and other successful chat products use separate AI services
- **Independent scaling:** Scale AI processing separately from web serving as you grow
- **Technology freedom:** Use the best tools for each job (Next.js for web, Node.js for AI)

**How it works:**
1. User types message → Next.js API route
2. Next.js saves user message → calls Agent Service via HTTP
3. Agent processes with LangChain → returns AI response
4. Next.js saves AI response → sends back to UI

**Shape:**
- Agent Service (Node.js + Express): `POST /chat`
  - Request: { conversation_id, messages: [{role, content}], client_metadata? }
  - Response (v1 buffered): { assistant_message: { id, role: "assistant", content, model, usage? }, timing_ms }
  - Later (v1.1): `GET /chat/stream` (SSE) for token streaming
- Frontend API route (Next.js): `/api/ai/chat`
  - Validates input, persists the user message → calls Agent Service → persists assistant reply → responds to UI
  - Keeps Supabase schema ownership on the web tier

### Data & Persistence
- Use existing tables for conversations and messages (from `frontend/supabase/migrations`).
- For forward compatibility, plan to include (now or soon):
  - `provider` (text), `model` (text)
  - `usage_prompt_tokens` (int), `usage_completion_tokens` (int), `usage_total_tokens` (int)
  - `latency_ms` (int)
  - `run_id` / `trace_url` (text) for LangSmith linking

We will attempt v1 without schema changes; if needed, add a small migration in a follow-up.

### UX Notes
- Minimal UX: show a loading/typing indicator; render final assistant message once ready.
- Ordering: ensure stable ordering by created_at and role.
- Empty/error states: surface a simple error toast if agent call fails.

### Telemetry & Observability
- LangSmith enabled by default.
  - Project: "chef chopsky"
  - Tags: ["web", "agent", env, model]
  - Metadata: conversation_id, latest_message_id, user_id (null for anon), model, latency_ms
- Structured logs at the Agent Service and the Next.js proxy for request start/end and errors.

### Configuration & Secrets
- Provider: OpenAI (default)
- Required env (local dev):
  - `OPENAI_API_KEY`
  - `LANGCHAIN_TRACING=true`
  - `LANGCHAIN_PROJECT=chef chopsky`
  - `LANGCHAIN_API_KEY` (if using LangSmith cloud)
  - Frontend Supabase envs (already present in frontend)
- Keep `.env.example` updated in both `agent/` and `frontend/`.

### Risks & Mitigations
- Agent cold start latency: mitigate with warm-up ping in dev; observe latency_ms.
- Error handling: ensure graceful fallback and user-visible error message.
- Schema drift: start with current schema; add migration only if necessary.

### Work Plan (Stories → Tasks)
1) Backend integration
   - Implement Agent Service `POST /chat` (Node runtime, Express/Fastify)
   - Add LangSmith tracing + OpenAI config in the agent
2) Frontend API proxy
   - Implement `/app/api/ai/chat/route.ts` to call Agent Service
   - Persist user and assistant messages in Supabase
3) UI wiring
   - Wire `ChatInterface` send to call `/api/ai/chat`
   - Show minimal loading indicator; render assistant reply
4) Configuration & Docs
   - Add `.env.example` entries (frontend/agent)
   - Document local dev: run agent service + Next.js
5) Validation
   - Add fast integration test: send a message → get assistant text → persisted
   - Manual smoke on local dev

### Definition of Done
- End-to-end demoable: user → message → agent → assistant reply shows up in UI
- Data persists correctly; basic logs + LangSmith traces present
- Anonymous flow works; no auth required
- Docs updated for running locally


